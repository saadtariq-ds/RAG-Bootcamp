{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de4bd0b",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10b2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3154063",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdbd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm = init_chat_model(model=\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0203403",
   "metadata": {},
   "source": [
    "## Document Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4dc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(file_path=\"langchain_crewai_dataset.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ddfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc64533",
   "metadata": {},
   "source": [
    "## Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6fab092",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"lambda_mult\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1578a2",
   "metadata": {},
   "source": [
    "## Query Decomposition Prompt and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db697c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\nYou are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\\n\\nQuestion: \"{question}\"\\n\\nSub-questions:\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "\n",
    "query_decomposition_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe454ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_chain = query_decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1beaf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are some sub-questions that break down the original complex question:\\n\\n1. **What types of memory mechanisms does LangChain utilize?**  (This focuses on understanding LangChain's memory capabilities)\\n2. **How does LangChain integrate agents into its framework?** (This explores LangChain's approach to agent-based functionality)\\n3. **What are the memory and agent capabilities of CrewAI?** (This establishes a baseline for comparison) \\n4. **What are the key differences in memory management and agent implementation between LangChain and CrewAI?** (This prompts a direct comparison) \\n\\n\\nBy answering these sub-questions, you can gain a comprehensive understanding of how LangChain and CrewAI handle memory and agents, allowing for a more informed comparison. \\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "\n",
    "decomposition_question = decomposition_chain.invoke(input={\"question\": query})\n",
    "\n",
    "decomposition_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8eb8c",
   "metadata": {},
   "source": [
    "## QA Prompt and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95657de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nUse the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2da4c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nUse the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002341CAF8D70>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002341D0E5A60>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = create_stuff_documents_chain(\n",
    "    llm=llm, \n",
    "    prompt=qa_prompt\n",
    ")\n",
    "\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dc74a",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba55bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke(input={\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-•1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for sub_question in sub_questions:\n",
    "        docs = retriever.invoke(sub_question)\n",
    "        result = qa_chain.invoke(input={\"input\": sub_question, \"context\": docs})\n",
    "        results.append(f\"Q: {sub_question}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03aa038",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29440c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions that break down the complex query:\n",
      "A: Please provide the complex query you would like to break down into sub-questions.  \n",
      "\n",
      "For example, you could ask: \n",
      "\n",
      "\"Here is a complex query: **How can I use CrewAI to analyze a legal contract for potential risks?** \n",
      "\n",
      "Here are some sub-questions that break down the complex query:\" \n",
      "\n",
      "\n",
      "I can then help you formulate relevant sub-questions based on the provided context and your query. \n",
      "\n",
      "\n",
      "\n",
      "Q: **What types of memory mechanisms does LangChain utilize?**\n",
      "A: According to the context, LangChain utilizes memory modules such as:\n",
      "\n",
      "* **ConversationBufferMemory**\n",
      "* **ConversationSummaryMemory** \n",
      "\n",
      "\n",
      "These modules help the LLM remember past conversation turns or summarize long interactions. \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents leverage memory for task completion?**\n",
      "A: LangChain agents use **context-aware memory** to aid in task completion. This means they can remember and utilize information from previous steps in a sequence, allowing them to make more informed decisions and maintain coherence throughout the task.  \n",
      "\n",
      "\n",
      "Q: **What memory and agent capabilities are offered by CrewAI?**\n",
      "A: While the provided context describes CrewAI's framework for structuring collaborative LLM agents, it doesn't explicitly mention specific memory capabilities or agent capabilities beyond:\n",
      "\n",
      "* **Defined purpose, goal, and tools:** Each agent operates with a clear objective, a desired outcome, and a set of resources to achieve it.\n",
      "* **Structured workflows:** Agents have designated roles (e.g., researcher, planner, executor) and work semi-independently within a defined process.\n",
      "* **Context sharing and communication:** Agents can share information and communicate dynamically to coordinate their efforts.\n",
      "\n",
      "To get a complete picture of CrewAI's memory and agent capabilities, you'd need to consult additional documentation or resources. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q: **What are the key differences in memory management and agent functionality between LangChain and CrewAI?**\n",
      "A: Based on the provided context, here's a breakdown of the key differences in memory management and agent functionality between LangChain and CrewAI:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Memory Management:** LangChain agents utilize \"context-aware memory\" which means they can remember and utilize information from previous steps in a sequence. This allows for more sophisticated and dynamic interactions.\n",
      "* **Agent Functionality:** LangChain agents operate on a \"planner-executor\" model.\n",
      "\n",
      "    * **Planner:**  Analyzes the goal and devises a plan involving tool invocations. This can include decision-making based on the current context and potentially branching logic.\n",
      "    * **Executor:**  Carries out the planned actions, interacting with tools as instructed.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Memory Management:** The context doesn't explicitly detail CrewAI's memory management strategies.  We can infer that it likely manages memory relevant to its role-based collaboration features. \n",
      "* **Agent Functionality:**  CrewAI focuses on \"role-based collaboration.\"  This suggests it might:\n",
      "\n",
      "    * **Assign roles:**  Different AI agents or modules could be assigned specific roles within a larger task.\n",
      "    * **Coordinate interactions:** CrewAI could manage communication and task delegation between these role-based agents.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "* **Focus:** LangChain emphasizes agent planning and tool execution with a strong emphasis on context-aware memory. CrewAI prioritizes collaborative workflows through role assignment and coordination.\n",
      "* **Explicit Memory Details:** LangChain clearly describes its context-aware memory, while CrewAI's memory management is less defined in the provided context.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions!\n",
      "\n",
      "\n",
      "Q: These sub-questions allow for more focused document retrieval by isolating specific aspects of memory and agent use in both LangChain and CrewAI\n",
      "A: The provided text explains how LangChain and CrewAI work together, focusing on their respective strengths:\n",
      "\n",
      "* **LangChain:** \n",
      "    *  Excels at **retrieval**, using both **keyword-based (sparse)** and **embedding-based (dense)** methods for comprehensive search within large document collections. \n",
      "    *  Integrates with vector databases to enable **semantic search**.\n",
      "    *  Handles **tool wrapping** in agent systems.\n",
      "* **CrewAI:**\n",
      "    *  Focuses on **role-based collaboration** in agent systems.\n",
      "\n",
      "The statement \"These sub-questions allow for more focused document retrieval...\" suggests that the sub-questions are designed to pinpoint specific information needs within LangChain or CrewAI's functionalities. \n",
      "\n",
      "\n",
      "Let me know if you have the actual sub-questions; I can then help you understand how they contribute to focused document retrieval within these systems. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "\n",
    "final_answer = full_query_decomposition_rag_pipeline(user_query=query)\n",
    "print(\"✅ Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1edb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
