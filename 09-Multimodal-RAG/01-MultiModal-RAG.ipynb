{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9535a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import fitz\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cda44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2d4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8b5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2c484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3a9e4",
   "metadata": {},
   "source": [
    "## Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_embedding(image_data):\n",
    "    \"\"\" \n",
    "    Embedding Images Data using CLIP\n",
    "    \"\"\"\n",
    "    if isinstance(image_data, str):  # If path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data\n",
    "\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "        # Normalizing Embeddings to unit vector\n",
    "        normalized_image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        return normalized_image_features.squeeze().numpy()\n",
    "    \n",
    "def text_embedding(text_data):\n",
    "    \"\"\" \n",
    "    Embedding Text Data using CLIP\n",
    "    \"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text_data,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77    # CLIP's max token length\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "        # Normalizing Embeddings to unit vector\n",
    "        normalized_text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return normalized_text_features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f9590",
   "metadata": {},
   "source": [
    "## Processing PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ae7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"multimodal_sample.pdf\"\n",
    "\n",
    "document = fitz.open(filename=pdf_path)\n",
    "\n",
    "all_documents = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25aae87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=500\n",
    "chunk_overlap=100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01845912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('multimodal_sample.pdf')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(document):\n",
    "    ## Process Text\n",
    "    text = page.get_text()\n",
    "    \n",
    "    if text.strip():\n",
    "        # Create temporary document for splitting\n",
    "        temporary_document = Document(\n",
    "            page_content=text, metadata={\"page\": i, \"type\":\"text\"}\n",
    "        )\n",
    "        text_chunks = text_splitter.split_documents(documents=[temporary_document])\n",
    "\n",
    "        # Embed Each Chunk using CLIP\n",
    "        for text_chunk in text_chunks:\n",
    "            embedding = text_embedding(text_data=text_chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_documents.append(text_chunk)\n",
    "\n",
    "    ## Process Images: Three Important Actions:\n",
    "    ## 1. Convert PDF image to PIL format\n",
    "    ## 2. Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ## 3. Create CLIP embedding for retrieval\n",
    "\n",
    "    for image_index, image in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = image[0]\n",
    "            base_image = document.extract_image(xref=xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "            # Create Unique Identifier\n",
    "            image_id = f\"page_{i}_img_{image_index}\"\n",
    "\n",
    "            # Store Image as base64 for later use with Model\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(fp=buffered, format=\"PNG\")\n",
    "            image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = image_base64\n",
    "\n",
    "            # Embed Image using CLIP\n",
    "            embedding = image_embedding(image_data=pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            # Create Document for Image\n",
    "            image_document = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\":i, \"type\":\"image\", \"image_id\":image_id}\n",
    "            )\n",
    "            all_documents.append(image_document)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06b9598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d104958",
   "metadata": {},
   "source": [
    "## Create Unified Vector Store with CLIP Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71183a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00267245,  0.01283001, -0.05183142, ..., -0.00385085,\n",
       "         0.02977719, -0.00010682],\n",
       "       [ 0.01732343, -0.01327688, -0.02427032, ...,  0.08994049,\n",
       "        -0.00272151,  0.0325304 ]], shape=(2, 512), dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84878bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x15e8001b1d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(document.page_content, embedding) for document, embedding in zip(all_documents, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[document.metadata for document in all_documents]\n",
    ")\n",
    "\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9ff31",
   "metadata": {},
   "source": [
    "## Create Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870b456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\" \n",
    "    Unified Retrieval using CLIP Embeddings for both text and images\n",
    "    \"\"\"\n",
    "    # Embed Query using CLIP\n",
    "    query_embedding = text_embedding(text_data=query)\n",
    "\n",
    "    # Search in Unified Vector Store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6894cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_documents):\n",
    "    \"\"\" \n",
    "    Create a message with both text and images for Model\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_documents = [document for document in retrieved_documents if document.metadata.get(\"type\") == \"text\"]\n",
    "    image_documents = [document for document in retrieved_documents if document.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_documents:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {text_document.metadata['page']}]: {text_document.page_content}\"\n",
    "            for text_document in text_documents\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for image_document in image_documents:\n",
    "        image_id = image_document.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {image_document.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5e278",
   "metadata": {},
   "source": [
    "## Create Multimodal RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc3a2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"\n",
    "    Main pipeline for multimodal RAG.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_documents = retrieve_multimodal(query=query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query=query, retrieved_documents=retrieved_documents)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(retrieved_documents)} documents:\")\n",
    "    for retrieved_document in retrieved_documents:\n",
    "        document_type = retrieved_document.metadata.get(\"type\", \"unknown\")\n",
    "        page = retrieved_document.metadata.get(\"page\", \"?\")\n",
    "        if document_type == \"text\":\n",
    "            preview = retrieved_document.page_content[:100] + \"...\" if len(retrieved_document.page_content) > 100 else retrieved_document.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9aaea1",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11b503e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What does the chart on page 1 show about revenue trends?\",\n",
    "    \"Summarize the main findings from the document\",\n",
    "    \"What visual elements are present in the document?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b96536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What does the chart on page 1 show about revenue trends?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The chart on page 1 shows that revenue increased steadily across Q1, Q2, and Q3. In Q1 (blue bar), revenue was the lowest but showed a moderate increase, likely due to new product lines. In Q2 (green bar), revenue grew further, driven by marketing campaigns. Q3 (red bar) had the highest revenue, indicating exponential growth attributed to global expansion. Overall, the chart demonstrates a consistent upward trend in revenue over the three quarters, with the largest jump occurring in Q3.\n",
      "======================================================================\n",
      "\n",
      "Query: Summarize the main findings from the document\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: **Main Findings Summary:**\n",
      "\n",
      "- Revenue increased steadily across Q1, Q2, and Q3.\n",
      "- **Q1** saw moderate growth, attributed to the introduction of new product lines.\n",
      "- **Q2** outperformed Q1, primarily due to successful marketing campaigns.\n",
      "- **Q3** experienced the highest and exponential growth, driven by global expansion.\n",
      "- The accompanying chart visually confirms the trend, with each quarter’s revenue higher than the last, and the most significant jump occurring in Q3.\n",
      "======================================================================\n",
      "\n",
      "Query: What visual elements are present in the document?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The visual elements present in the document include:\n",
      "\n",
      "1. **Bar Chart**:\n",
      "   - The image shows a simple bar chart with three vertical bars.\n",
      "     - The first bar is **blue** (representing Q1, as inferred from the context).\n",
      "     - The second bar is **green** (representing Q2).\n",
      "     - The third bar is **red** (representing Q3).\n",
      "   - The bars increase in height from left to right, visually illustrating the steady and significant revenue growth described in the text.\n",
      "\n",
      "2. **Color Usage**:\n",
      "   - Distinct colors (blue, green, red) are used for each quarter to differentiate between Q1, Q2, and Q3 revenue visually.\n",
      "\n",
      "These elements together help visually communicate the document's key point: revenue increased across each quarter, with the highest in Q3.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    answer = multimodal_pdf_rag_pipeline(query)\n",
    "    \n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
