{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2e073a",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05217391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a6834c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a141f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "llm = init_chat_model(model=\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc24073",
   "metadata": {},
   "source": [
    "## Custom Semantic Chunker with Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split(self, text: str):\n",
    "        sentences = [sentence.strip() for sentence in text.split(\"\\n\") if sentence.strip()]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "\n",
    "        for sentence in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(\n",
    "                X=[embeddings[sentence - 1]],\n",
    "                Y=[embeddings[sentence]]\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity >= self.threshold:\n",
    "                current_chunk.append(sentences[sentence])\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk) + \".\")\n",
    "                current_chunk=[sentences[sentence]]\n",
    "\n",
    "        chunks.append(\" \".join(current_chunk) + \".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, documents):\n",
    "        result = []\n",
    "        for document in documents:\n",
    "            for chunk in self.split(document.page_content):\n",
    "                result.append(Document(page_content=chunk, metadata=document.metadata))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913d1ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=' \\nLangChain is a framework for building applications with LLMs.\\nLangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"\"\" \n",
    "LangChain is a framework for building applications with LLMs.\n",
    "LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination\n",
    "\"\"\"\n",
    "\n",
    "document = Document(page_content=sample_text)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01483cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs. LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone..'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory and retrievers..'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris..'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = ThresholdSemanticChunker(threshold=0.7)\n",
    "\n",
    "chunks = chunker.split_documents([document])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d946f81",
   "metadata": {},
   "source": [
    "## Storing Data into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e17932",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b09fa",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e65a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=' Answer the question based on the following context:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\" Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abea82",
   "metadata": {},
   "source": [
    "## Creating RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4841182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db82cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework for building applications that use large language models (LLMs). It offers modular abstractions—such as chains, agents, memory, and retrievers—that let developers combine LLMs with tools like OpenAI and Pinecone to create more complex, reusable AI workflows.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {\"question\": \"What is Langchain used for?\"}\n",
    "\n",
    "result = rag_chain.invoke(input=question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde4b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
