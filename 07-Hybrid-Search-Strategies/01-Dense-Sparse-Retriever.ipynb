{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94ed16e",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11284bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8b4567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c875be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d81990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm = init_chat_model(model=\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f48275",
   "metadata": {},
   "source": [
    "## Create Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59baa883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain helps build LLM application'),\n",
       " Document(metadata={}, page_content='Pinecone is a vector database for semantic search'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris'),\n",
       " Document(metadata={}, page_content='LangChain can be used to develop Agentic AI applications'),\n",
       " Document(metadata={}, page_content='LangChain has many types of retrievers')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    Document(page_content=\"LangChain helps build LLM application\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris\"),\n",
    "    Document(page_content=\"LangChain can be used to develop Agentic AI applications\"),\n",
    "    Document(page_content=\"LangChain has many types of retrievers\"),\n",
    "]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4110cec",
   "metadata": {},
   "source": [
    "## Create Dense Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32cfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "dense_retriever = dense_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f11a36",
   "metadata": {},
   "source": [
    "## Create Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "488ae010",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_retriever = BM25Retriever.from_documents(\n",
    "    documents=documents\n",
    ")\n",
    "\n",
    "sparse_retriever.k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e65a65",
   "metadata": {},
   "source": [
    "## Combine with Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbb01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec1dab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FB245BCEF0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001FB25AFBC50>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff0221",
   "metadata": {},
   "source": [
    "## Query Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abcd655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ðŸ”¶ Document 1:\n",
      "LangChain helps build LLM application\n",
      "\n",
      " ðŸ”¶ Document 2:\n",
      "LangChain can be used to develop Agentic AI applications\n",
      "\n",
      " ðŸ”¶ Document 3:\n",
      "LangChain has many types of retrievers\n",
      "\n",
      " ðŸ”¶ Document 4:\n",
      "Pinecone is a vector database for semantic search\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I build an application using LLMs?\"\n",
    "\n",
    "results = hybrid_retriever.invoke(input=query)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n ðŸ”¶ Document {i + 1}:\\n{result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1bb9bd",
   "metadata": {},
   "source": [
    "## Create RAG Pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b54764",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the context below.\n",
    "                                      \n",
    "Context: {context}\n",
    "                                      \n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82afe12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57bee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FB245BCEF0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001FB25AFBC50>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=' \\nAnswer the question based on the context below.\\n\\nContext: {context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FB235836B0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FB24E7B830>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba2560",
   "metadata": {},
   "source": [
    "## Query RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58244c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " ## Quickâ€‘Start Guide: Building an LLMâ€‘Powered App with LangChain & Pinecone\n",
      "\n",
      "Below is a practical, stepâ€‘byâ€‘step recipe that pulls together everything from the context:\n",
      "\n",
      "| Step | What youâ€™ll do | Why it matters |\n",
      "|------|----------------|----------------|\n",
      "| 1 | **Pick an LLM** (OpenAI, Anthropic, Cohere, etc.) | The core â€œbrainâ€ that generates text. |\n",
      "| 2 | **Create a LangChain environment** | LangChain gives you reusable building blocks: prompts, chains, agents, and retrievers. |\n",
      "| 3 | **Add a semantic retriever** (e.g., `VectorStoreRetriever`) | Pulls in relevant documents before the LLM answers. |\n",
      "| 4 | **Connect to Pinecone** | Fast, scalable vector database for semantic search. |\n",
      "| 5 | **Wrap everything in an Agent** | Lets the system decide *what* to do (search, ask, write, etc.) in a loop. |\n",
      "| 6 | **Deploy** (FastAPI + Docker or serverless) | Exposes your app to users. |\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Pick Your LLM\n",
      "\n",
      "```bash\n",
      "pip install openai  # or anthropic, cohere, etc.\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "llm = OpenAI(temperature=0.7, model=\"gpt-4o-mini\")\n",
      "```\n",
      "\n",
      "> **Tip** â€“ If you want to stay openâ€‘source, try `llama.cpp` or `Mistral` through the `langchain.llms` interface.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Set Up LangChain\n",
      "\n",
      "```bash\n",
      "pip install langchain langchain-openai langchain-community\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "```\n",
      "\n",
      "Create a reusable prompt:\n",
      "\n",
      "```python\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\", \"context\"],\n",
      "    template=\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Question:\n",
      "{question}\n",
      "\n",
      "Answer:\"\"\"\n",
      ")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Semantic Retriever with Pinecone\n",
      "\n",
      "#### a. Create a Pinecone index\n",
      "\n",
      "```bash\n",
      "pip install pinecone-client\n",
      "```\n",
      "\n",
      "```python\n",
      "import pinecone\n",
      "pinecone.init(api_key=\"YOUR_PINECONE_KEY\", environment=\"us-east1-gcp\")\n",
      "\n",
      "# Create an index if it doesnâ€™t exist\n",
      "if \"my-index\" not in pinecone.list_indexes():\n",
      "    pinecone.create_index(\n",
      "        name=\"my-index\",\n",
      "        dimension=1536,  # same as your embedding model\n",
      "        metric=\"cosine\",\n",
      "    )\n",
      "```\n",
      "\n",
      "#### b. Ingest documents\n",
      "\n",
      "```python\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Pinecone\n",
      "\n",
      "# Load a sample text file\n",
      "loader = TextLoader(\"sample.txt\")\n",
      "docs = loader.load()\n",
      "\n",
      "# Embed and store\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = Pinecone.from_documents(docs, embeddings, index_name=\"my-index\")\n",
      "```\n",
      "\n",
      "#### c. Build a retriever\n",
      "\n",
      "```python\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Combine Retriever + LLM into a Chain\n",
      "\n",
      "```python\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",          # concatenate retrieved docs\n",
      "    retriever=retriever,\n",
      "    prompt=prompt,\n",
      ")\n",
      "```\n",
      "\n",
      "Now `qa_chain({\"question\": \"What is LangChain?\"})` will:\n",
      "\n",
      "1. Search Pinecone for 4 relevant vectors.\n",
      "2. Feed the retrieved text + question into the prompt.\n",
      "3. Return the LLMâ€™s answer.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Wrap It All in an Agent (Optional but Powerful)\n",
      "\n",
      "If you want the app to decide *whether* to search, ask, or generate a response, use LangChainâ€™s **Agent**:\n",
      "\n",
      "```python\n",
      "from langchain.agents import initialize_agent, Tool, AgentType\n",
      "\n",
      "# Define a tool that uses the QA chain\n",
      "qa_tool = Tool(\n",
      "    name=\"SemanticSearchQA\",\n",
      "    func=lambda q: qa_chain({\"question\": q})[\"result\"],\n",
      "    description=\"Use this to answer questions with upâ€‘toâ€‘date knowledge from the Pinecone index.\"\n",
      ")\n",
      "\n",
      "# Add more tools as needed (e.g., web search, database queries)\n",
      "\n",
      "agent = initialize_agent(\n",
      "    tools=[qa_tool],\n",
      "    llm=llm,\n",
      "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True,\n",
      ")\n",
      "```\n",
      "\n",
      "Now you can ask the agent anything:\n",
      "\n",
      "```python\n",
      "response = agent.run(\"Explain how LangChain helps build LLM applications.\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "The agent will:\n",
      "\n",
      "1. Interpret the query.\n",
      "2. Decide to use the `SemanticSearchQA` tool.\n",
      "3. Return a polished answer.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Expose the App (FastAPI Example)\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Query(BaseModel):\n",
      "    question: str\n",
      "\n",
      "@app.post(\"/ask\")\n",
      "async def ask(query: Query):\n",
      "    try:\n",
      "        answer = qa_chain({\"question\": query.question})[\"result\"]\n",
      "        return {\"answer\": answer}\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "```\n",
      "\n",
      "Run:\n",
      "\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "You now have an endpoint `/ask` that accepts JSON `{\"question\": \"...\"}` and returns an LLMâ€‘powered answer backed by Pinecone semantic search.\n",
      "\n",
      "---\n",
      "\n",
      "## Recap: The Flow\n",
      "\n",
      "1. **LLM** â†’ Generates text.\n",
      "2. **LangChain** â†’ Orchestrates prompts, chains, agents.\n",
      "3. **Retriever** â†’ Pulls relevant docs from Pinecone.\n",
      "4. **Pinecone** â†’ Stores and searches embeddings quickly.\n",
      "5. **Agent** (optional) â†’ Decides which tool to use.\n",
      "6. **FastAPI** â†’ Serves the app to users.\n",
      "\n",
      "With this architecture you can:\n",
      "\n",
      "- **Add more data** (reâ€‘index Pinecone) without touching the code.\n",
      "- **Swap LLMs** by changing the `llm` instantiation.\n",
      "- **Scale**: Pinecone handles millions of vectors; LangChain can run locally or on cloud GPUs.\n",
      "\n",
      "Happy building! ðŸš€\n",
      "\n",
      "ðŸ“ƒSource Documents:\n",
      "\n",
      "Result 1: LangChain helps build LLM application\n",
      "\n",
      "Result 2: LangChain can be used to develop Agentic AI applications\n",
      "\n",
      "Result 3: LangChain has many types of retrievers\n",
      "\n",
      "Result 4: Pinecone is a vector database for semantic search\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I build an app using LLMs\"\n",
    "\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(f\"âœ… Answer:\\n {response[\"answer\"]}\")\n",
    "\n",
    "print(\"\\nðŸ“ƒSource Documents:\")\n",
    "for i, result in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nResult {i+1}: {result.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476c900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
