{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0813fd73",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7458968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9076a8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceacdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca018a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm = init_chat_model(model=\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96ff12",
   "metadata": {},
   "source": [
    "## Load and Chunk the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21b137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='The framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.\\nLangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='BM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.\\nFAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content=\"The 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\\nThe 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\"),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='MMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.\\nTool usage in LangChain allows agents to execute predefined Python functions with contextual input from the user.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(file_path=\"langchain_rag_dataset.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cb72c",
   "metadata": {},
   "source": [
    "## Create Vector Store and MMR Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97ba968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B822359A00>, search_type='mmr', search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fb357",
   "metadata": {},
   "source": [
    "## Prompt and Rag Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7917ee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context provided.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1219ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbda2d3",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf5ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " LangChain supports agents in several ways:\n",
      "\n",
      "* **Tool integration:** Agents can utilize tools like calculators, search APIs, or custom functions based on their instructions.\n",
      "* **External API and database interaction:** Agents can connect with external APIs and databases, expanding their capabilities beyond text-based processing.\n",
      "* **Tool selection and execution:** LangChain allows LLMs to act as agents, deciding which tool to use and in what sequence to complete a task.\n",
      "\n",
      "For memory, LangChain offers:\n",
      "\n",
      "* **Conversational memory:**  `ConversationBufferMemory`  retains previous interactions for more coherent multi-turn conversations.\n",
      "* **Summarization memory:** `ConversationSummaryMemory`  provides a summarized view of past interactions, useful for longer conversations. \n",
      "\n",
      "\n",
      "Together, these features enable LangChain to build sophisticated, context-aware agents capable of handling complex tasks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain support agents and memory?\"\n",
    "\n",
    "response = rag_chain.invoke(input={\"input\": query})\n",
    "\n",
    "print(\"✅ Answer:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ec73f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does LangChain support agents and memory?',\n",
       " 'context': [Document(id='e9d7ca86-ef61-4d5d-900c-39175d548e19', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       "  Document(id='f5b54435-89aa-4d40-a19e-d6a445daf3a9', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.'),\n",
       "  Document(id='45f027da-bf57-4973-a4e1-3326fb364caf', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.')],\n",
       " 'answer': 'LangChain supports agents in several ways:\\n\\n* **Tool integration:** Agents can utilize tools like calculators, search APIs, or custom functions based on their instructions.\\n* **External API and database interaction:** Agents can connect with external APIs and databases, expanding their capabilities beyond text-based processing.\\n* **Tool selection and execution:** LangChain allows LLMs to act as agents, deciding which tool to use and in what sequence to complete a task.\\n\\nFor memory, LangChain offers:\\n\\n* **Conversational memory:**  `ConversationBufferMemory`  retains previous interactions for more coherent multi-turn conversations.\\n* **Summarization memory:** `ConversationSummaryMemory`  provides a summarized view of past interactions, useful for longer conversations. \\n\\n\\nTogether, these features enable LangChain to build sophisticated, context-aware agents capable of handling complex tasks. \\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d42482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
